base_path: "/mnt/data"

training_config:
  is_training: true     # Enable/disable training mode
  is_optimizing: true
  is_inference: false
  is_visualizing: false

optuna:
  n_trials: 20  # 20
  trial_epochs: 5  # 5
  eval_epochs: 20  # 20

inference:
  run_output_dir: "/home/tomo/Documents/deepfake-detection/outputs/2024-12-17/02-23-25_test/"
  batch_size: 256
  is_visualization: true

data:
  text_feature:
    use: true
    method: BERT  # TF-IDF, LSTM, BERT
  image_feature:
    use: true
    method: ResNet-50  # VGG-16, DenseNet-201, ViT-L/32, ResNet-50
  caption_feature:
    use: true
    method: BERT  # TF-IDF, LSTM, BERT, BERT+BLIP

  train_data: "train_fake_news.csv"
  val_data: "val_fake_news.csv"
  test_data: "test_fake_news.csv"
  batch_size: 256

model:
  type: multi_modal  # multi_modal, multi_modal_with_caption, multi_modal_with_caption_using_cross_attention
  input_size:
    text: 768
    image: 2048
    caption: 768
  hidden_size: 512
  dropout_rate: 0.3
  output_size: 1
  num_heads: 3  # Used for Cross-Attention (if applicable)

training:
  learning_rate: 1e-3
  patience: 3
  num_epochs: 1
